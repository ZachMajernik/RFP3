{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f7f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium renderlab\n",
    "# !pip install opencv-python\n",
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74465ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "%config NotebookApp.iopub_msg_rate_limit=10000\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d3f34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "#visualise maze:\n",
    "rfpMaze = [\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "maze1 = [\"SFFH\", \"FHHF\", \"FHFG\", \"FFFH\", \"HFHH\"]\n",
    "\n",
    "desc = rfpMaze\n",
    "mazeSize = [len(desc),len(desc[0])]\n",
    "\n",
    "statePositions = [[] for _ in range(mazeSize[0])]\n",
    "stateNum = 0\n",
    "for i in range(mazeSize[0]):\n",
    "    for j in range(mazeSize[1]):\n",
    "        statePositions[i].append(stateNum)\n",
    "        stateNum += 1\n",
    "        \n",
    "\n",
    "        \n",
    "giftState = -1\n",
    "gift_found = False\n",
    "for i in range(len(desc)):\n",
    "    if gift_found:\n",
    "        break\n",
    "    for j in range(len(desc[i])):\n",
    "        giftState += 1\n",
    "        if desc[i][j] == 'G':\n",
    "            gift_found = True\n",
    "            break\n",
    "            \n",
    "print(giftState)\n",
    "print(statePositions)\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x5\", is_slippery=False, render_mode=\"human\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b980268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetTable():\n",
    "    global qTable_1\n",
    "    qTable_1 = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        qTable_1[i] = [0,0,0,0] \n",
    "    global currentState\n",
    "    currentState = 0\n",
    "    \n",
    "def getPosition(state):\n",
    "    for i in range(len(statePositions)):\n",
    "        for j in range(len(statePositions[i])):\n",
    "            if statePositions[i][j] == state:\n",
    "                return i, j\n",
    "            \n",
    "def calcReward(state, nextState):\n",
    "    y1, x1 = getPosition(state)\n",
    "    y2, x2 = getPosition(nextState)\n",
    "    y3, x3 = getPosition(giftState)\n",
    "    \n",
    "    currentDist = (((y3 - y1)**2)+((x3 - x1)**2))**0.5\n",
    "    nextDist = (((y3 - y2)**2)+((x3 - x2)**2))**0.5\n",
    "    \n",
    "    changeInDist = currentDist-nextDist\n",
    "    return changeInDist\n",
    "#     if changeInDist > 0:\n",
    "#         return changeInDist/2\n",
    "#     else:\n",
    "#         return changeInDist\n",
    "\n",
    "def calcPossibleMoves(state):\n",
    "    global qTable_1\n",
    "    possibleMoves = []\n",
    "    \n",
    "    if state == 0:\n",
    "        return [1,2]\n",
    "    \n",
    "    if (state+1) % mazeSize[1] != 0:\n",
    "        possibleMoves.append(2)\n",
    "        \n",
    "    if (state+1) % mazeSize[1] != 1:\n",
    "        possibleMoves.append(0)\n",
    "        \n",
    "    if state > (mazeSize[1]-1):\n",
    "        possibleMoves.append(3)\n",
    "    \n",
    "    if state < ((mazeSize[0] * mazeSize[1]) - mazeSize[1]):\n",
    "        possibleMoves.append(1)\n",
    "        \n",
    "    return possibleMoves\n",
    "\n",
    "def nextStep(state):\n",
    "    global qTable_1\n",
    "    possMoves = calcPossibleMoves(state)\n",
    "    \n",
    "    if random.random() < epsilonValue:\n",
    "        nextMove = random.choice(possMoves)\n",
    "    else:\n",
    "        qValues = {}\n",
    "        for move in possMoves:\n",
    "            qValues[move] = qTable_1[state][move]\n",
    "\n",
    "        maxValue = max(qValues.values())\n",
    "        minValue = min(qValues.values())\n",
    "        count_max = sum(1 for value in qValues.values() if value == maxValue)\n",
    "        count_min = sum(1 for value in qValues.values() if value == minValue)\n",
    "\n",
    "        if count_max > 1 and count_max < len(possMoves):\n",
    "            nextMove = random.choice([move for move in possMoves if qValues[move] != minValue])\n",
    "        elif count_max == len(possMoves):\n",
    "            nextMove = random.choice(possMoves)\n",
    "        else:\n",
    "            nextMove = max(qValues, key=qValues.get)\n",
    "    return nextMove\n",
    "\n",
    "def pathFound():\n",
    "    currentState = 0\n",
    "    for i in range(mazeSize[0]*mazeSize[1]-1):\n",
    "        bestDirection = None\n",
    "        if max(qTable_1[currentState]) > 0:\n",
    "            bestDirection = qTable_1[currentState].index(max(qTable_1[currentState]))\n",
    "        newState = 0\n",
    "        if bestDirection == 0 and 0 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - 1\n",
    "        elif bestDirection == 1 and 1 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + mazeSize[1]\n",
    "        elif bestDirection == 2 and 2 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + 1\n",
    "        elif bestDirection == 3 and 3 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - mazeSize[1]\n",
    "\n",
    "        if newState == giftState:\n",
    "            return True\n",
    "        currentState = newState\n",
    "    return False\n",
    "\n",
    "def updateTable_qLearning(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (gamma*max(qTable_1[nextState])) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ\n",
    "\n",
    "def updateTable_sarsa(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    nextDirection = nextStep(nextState)\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (gamma*qTable_1[nextState][nextDirection]) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ, nextDirection\n",
    "\n",
    "def updateTable_monteCarlo(steps, totalReward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    minChangeInQ = 0\n",
    "    didConverge = False\n",
    "    avgReward = totalReward/len(steps)\n",
    "    for i in range(len(steps)):\n",
    "        updated = qTable_1[steps[i][0]][steps[i][1]] + alpha*(avgReward-qTable_1[steps[i][0]][steps[i][1]])\n",
    "        changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "        if i == 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        elif changeInQ < minChangeInQ and changeInQ != 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        if changeInQ < convergenceThresh:\n",
    "            if changeInQ > 0 and pathFound():\n",
    "                didConverge = True\n",
    "        qTable_1[steps[i][0]][steps[i][1]] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    return didConverge, minChangeInQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23b6b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilonValue = 0.3\n",
    "alpha = 0.6\n",
    "gamma = 0.7\n",
    "convergenceThresh = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93fee2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 27/1000\n",
      "Time: 46.576 sec\n",
      "Q-Table:\n",
      "0: [0, 3.6696042944978684, 0.37012405301938683, 0]\n",
      "1: [-0.2232507980078372, -0.6, 0.1034087255188349, 0]\n",
      "2: [-0.19284075771494588, 0, -0.07386337537059635, 0]\n",
      "3: [0, -0.6, 0, 0]\n",
      "4: [0, 4.074683756398308, -0.9359999999999999, -0.3855055314579773]\n",
      "5: [0, 0, 0, 0]\n",
      "6: [0, 0, 0, 0]\n",
      "7: [0, 0, 0, 0]\n",
      "8: [0, -0.9359999999999999, 4.755880094045253, -0.32823370037260663]\n",
      "9: [-0.48323048113532796, 5.98859305431757, 0.1741615110802449, -0.9359999999999999]\n",
      "10: [-0.35835600403794804, -0.6, 0.008810211512103594, -0.6]\n",
      "11: [0, 0.49311264907601676, 0, -0.6]\n",
      "12: [0, 0, 0, 0]\n",
      "13: [0, 7.410654037534688, 0, 1.827949307858085]\n",
      "14: [0, 0, 0, 0]\n",
      "15: [-0.6, 0, 0, 0]\n",
      "16: [0, 0, 0, -0.6]\n",
      "17: [-0.6, 0, 9.999580569599999, 0]\n",
      "18: [0, 0, 0, 0]\n",
      "19: [0, 0, 0, 0]\n",
      "change in Q: 0.00063\n",
      "46.575 seconds to converge\n"
     ]
    }
   ],
   "source": [
    "#Q LEARNING-----------------------------------------------------------------------\n",
    "\n",
    "qTable_1 = {}\n",
    "currentState = 0\n",
    "\n",
    "maxEpisodes = 1000\n",
    "currentEpisode = 1\n",
    "converged = False\n",
    "revistPenalty = -0.25\n",
    "\n",
    "timesVisited = {}\n",
    "def resetVisited():\n",
    "    global timesVisited\n",
    "    timesVisited = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        timesVisited[i] = 0\n",
    "resetVisited()\n",
    "\n",
    "resetTable()\n",
    "env.reset()\n",
    "start_time = time.time()\n",
    "while currentEpisode <= maxEpisodes:\n",
    "    global currentSteps\n",
    "    global currentState\n",
    "    if converged:\n",
    "        break\n",
    "        \n",
    "    direction = nextStep(currentState)\n",
    "    nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "    \n",
    "    if terminated:\n",
    "        if reward < 1:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 10\n",
    "            \n",
    "    if not terminated:\n",
    "        reward = calcReward(currentState, nextState)\n",
    "        if timesVisited[nextState] > 0:\n",
    "            reward += revistPenalty*timesVisited[nextState]\n",
    "    \n",
    "    converged, changeInQ = updateTable_qLearning(direction, nextState, reward)\n",
    "    \n",
    "\n",
    "    if terminated or truncated or converged:\n",
    "        observation, info = env.reset()\n",
    "        currentState = 0\n",
    "        if not converged:\n",
    "            currentEpisode += 1\n",
    "            resetVisited()\n",
    "\n",
    "        \n",
    "    if converged:\n",
    "        end_time = time.time()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "    print(\"Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "    print(\"Q-Table:\")\n",
    "    for i in range(len(qTable_1)):\n",
    "        print(str(i) + \": \" + str(qTable_1[i]))\n",
    "    print(\"change in Q: \" + str(changeInQ))\n",
    "            \n",
    "if converged:\n",
    "    duration = end_time - start_time\n",
    "    print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "else:\n",
    "    print(\"No convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07c94f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 35/1000\n",
      "Time: 69.623 sec\n",
      "Q-Table:\n",
      "0: [0.3658155902884366, 2.6735773180918163, -0.10381392992044942, 1.5405139983510374]\n",
      "1: [-0.7780273679403499, -0.9359999999999999, -0.56719064498439, 0]\n",
      "2: [-0.9143682284389549, -0.9743999999999999, -0.9355428912593289, 0]\n",
      "3: [-0.7628187255962103, -0.9743999999999999, 0, 0]\n",
      "4: [0, 3.1993472659412046, -0.9743999999999999, -0.994340132584316]\n",
      "5: [0, 0, 0, 0]\n",
      "6: [0, 0, 0, 0]\n",
      "7: [0, 0, 0, 0]\n",
      "8: [0, -0.6, 3.985384634686431, -1.3896562198677944]\n",
      "9: [-0.20572789330552788, 5.54150716523432, 0.12015962693980325, -0.6]\n",
      "10: [0, 0, -0.14164078649987388, -0.84]\n",
      "11: [0, 0.49311264907601676, 0, 0]\n",
      "12: [0, 0, 0, 0]\n",
      "13: [0, 7.303578609064932, -0.9359999999999999, -0.5355331873363428]\n",
      "14: [0, 0, 0, 0]\n",
      "15: [-0.6, 0, 0, 0]\n",
      "16: [0, 0, 0, -0.6]\n",
      "17: [-0.6, 0, 9.999580569599999, -0.751328137423857]\n",
      "18: [0, 0, 0, 0]\n",
      "19: [0, 0, 0, 0]\n",
      "change in Q: 0.00063\n",
      "69.622 seconds to converge\n"
     ]
    }
   ],
   "source": [
    "#SARSA-----------------------------------------------------------------------\n",
    "\n",
    "qTable_1 = {}\n",
    "currentState = 0\n",
    "\n",
    "maxEpisodes = 1000\n",
    "currentEpisode = 1\n",
    "converged = False\n",
    "revistPenalty = -0.25\n",
    "startOfEpisode = True\n",
    "direction = None\n",
    "\n",
    "timesVisited = {}\n",
    "def resetVisited():\n",
    "    global timesVisited\n",
    "    timesVisited = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        timesVisited[i] = 0\n",
    "resetVisited()\n",
    "\n",
    "resetTable()\n",
    "env.reset()\n",
    "start_time = time.time()\n",
    "while currentEpisode <= maxEpisodes:\n",
    "    global startOfEpisode\n",
    "    global currentSteps\n",
    "    global Direction\n",
    "    global currentState\n",
    "    if converged:\n",
    "        break\n",
    "        \n",
    "    if startOfEpisode:\n",
    "        direction = nextStep(currentState)\n",
    "        startOfEpisode = False\n",
    "    nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "    \n",
    "    if terminated:\n",
    "        if reward < 1:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 10\n",
    "            \n",
    "    if not terminated:\n",
    "        reward = calcReward(currentState, nextState)\n",
    "        if timesVisited[nextState] > 0:\n",
    "            reward += revistPenalty*timesVisited[nextState]\n",
    "    \n",
    "    converged, changeInQ, nextDirection = updateTable_sarsa(direction, nextState, reward)\n",
    "    direction = nextDirection\n",
    "\n",
    "    if terminated or truncated or converged:\n",
    "        observation, info = env.reset()\n",
    "        currentState = 0\n",
    "        if not converged:\n",
    "            currentEpisode += 1\n",
    "            startOfEpisode = False\n",
    "            resetVisited()\n",
    "\n",
    "        \n",
    "    if converged:\n",
    "        end_time = time.time()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "    print(\"Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "    print(\"Q-Table:\")\n",
    "    for i in range(len(qTable_1)):\n",
    "        print(str(i) + \": \" + str(qTable_1[i]))\n",
    "    print(\"change in Q: \" + str(changeInQ))\n",
    "            \n",
    "if converged:\n",
    "    duration = end_time - start_time\n",
    "    print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "else:\n",
    "    print(\"No convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6db2d001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 529/1000\n",
      "Current Path: [[0, 2]]\n",
      "Time: 578.855 sec\n",
      "Q-Table:\n",
      "0: [0, 3.713417989815291, 4.512665604696477, 0]\n",
      "1: [1.0287547378324782, 4.536095873541259, 2.844120224008705, 0]\n",
      "2: [1.7200539295104378, 1.9734927370112612, 0.13377512583005063, 0]\n",
      "3: [0.2181179281931227, -0.06862801462284593, 0, 0]\n",
      "4: [0, 3.570807365431569, 4.991346211596364, 1.5067997512346354]\n",
      "5: [0, 0, 0, 0]\n",
      "6: [0, 0, 0, 0]\n",
      "7: [0, 0, 0, 0]\n",
      "8: [0, 3.57204087544088, 1.3294706739586741, 0.2855729586780779]\n",
      "9: [0.28754401244938843, 1.5319654987472726, 0.28509545133638914, 0.3489283205766609]\n",
      "10: [0.2850972116718673, 0.324956768511563, 0.3442993992021014, 0.22035275812607807]\n",
      "11: [0, 0, 0, 0.3442993992021014]\n",
      "12: [0, 0, 0, 0]\n",
      "13: [0.23324629049204493, 1.8474769534611593, 0.5579293010304442, 0.4439004368776891]\n",
      "14: [0, 0, 0, 0]\n",
      "15: [0, 0, 0, 0]\n",
      "16: [0, 0, 0, 0.6408707170592368]\n",
      "17: [0.6408707170592368, 0, 1.510834782178534, 1.2021273146979545]\n",
      "18: [0, 0, 0, 0]\n",
      "19: [0, 0, 0, 0]\n",
      "change in Q: 3.57081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m direction \u001b[38;5;241m=\u001b[39m nextStep(currentState)\n\u001b[0;32m     36\u001b[0m episodeSteps\u001b[38;5;241m.\u001b[39mappend([currentState, direction])\n\u001b[1;32m---> 37\u001b[0m nextState, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(direction)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:308\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_gui(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:432\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    430\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m    431\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    435\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    436\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Monte Carlo -----------------------------------------------------------------------\n",
    "\n",
    "epsilonValue = 0.3\n",
    "alpha = 0.6\n",
    "gamma = 0.7\n",
    "convergenceThresh = 0.001\n",
    "\n",
    "currentState = 0\n",
    "qTable_1 = {}\n",
    "maxEpisodes = 1000\n",
    "currentEpisode = 1\n",
    "converged = False\n",
    "revistPenalty = -0.25\n",
    "\n",
    "episodeSteps = []\n",
    "totalReward = 0\n",
    "\n",
    "timesVisited = {}\n",
    "def resetVisited():\n",
    "    global timesVisited\n",
    "    timesVisited = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        timesVisited[i] = 0\n",
    "resetVisited()\n",
    "\n",
    "resetTable()\n",
    "env.reset()\n",
    "start_time = time.time()\n",
    "while currentEpisode <= maxEpisodes:\n",
    "    global currentSteps\n",
    "    global currentState\n",
    "    if converged:\n",
    "        break\n",
    "        \n",
    "    direction = nextStep(currentState)\n",
    "    episodeSteps.append([currentState, direction])\n",
    "    nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "    \n",
    "    if terminated:\n",
    "        if reward < 1:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 10\n",
    "            \n",
    "    if not terminated:\n",
    "        reward = calcReward(currentState, nextState)\n",
    "        if timesVisited[nextState] > 0:\n",
    "            reward += revistPenalty*timesVisited[nextState]\n",
    "            \n",
    "    totalReward += reward\n",
    "    currentState = nextState\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        converged, changeInQ = updateTable_monteCarlo(episodeSteps, totalReward)\n",
    "        episodeSteps = []\n",
    "        currentState = 0\n",
    "        currentEpisode += 1\n",
    "        resetVisited()\n",
    "\n",
    "        \n",
    "    if converged:\n",
    "        end_time = time.time()\n",
    "        observation, info = env.reset()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "    print(\"Current Path: \" + str(episodeSteps))\n",
    "    print(\"Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "    print(\"Q-Table:\")\n",
    "    for i in range(len(qTable_1)):\n",
    "        print(str(i) + \": \" + str(qTable_1[i]))\n",
    "    print(\"change in Q: \" + str(changeInQ))\n",
    "            \n",
    "if converged:\n",
    "    duration = end_time - start_time\n",
    "    print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "else:\n",
    "    print(\"No convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82da0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f0a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
