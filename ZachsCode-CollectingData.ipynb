{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f7f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium renderlab\n",
    "# !pip install opencv-python\n",
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74465ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "%config NotebookApp.iopub_msg_rate_limit=10000\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d3f34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "#visualise maze:\n",
    "rfpMaze = [\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "maze1 = [\"SFFH\", \"FHHF\", \"FHFG\", \"FFFH\", \"HFHH\"]\n",
    "\n",
    "desc = rfpMaze\n",
    "mazeSize = [len(desc),len(desc[0])]\n",
    "\n",
    "statePositions = [[] for _ in range(mazeSize[0])]\n",
    "stateNum = 0\n",
    "for i in range(mazeSize[0]):\n",
    "    for j in range(mazeSize[1]):\n",
    "        statePositions[i].append(stateNum)\n",
    "        stateNum += 1\n",
    "        \n",
    "\n",
    "        \n",
    "giftState = -1\n",
    "gift_found = False\n",
    "for i in range(len(desc)):\n",
    "    if gift_found:\n",
    "        break\n",
    "    for j in range(len(desc[i])):\n",
    "        giftState += 1\n",
    "        if desc[i][j] == 'G':\n",
    "            gift_found = True\n",
    "            break\n",
    "            \n",
    "print(giftState)\n",
    "print(statePositions)\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x5\", is_slippery=False, render_mode=\"human\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b980268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qTable_1 = {}\n",
    "def resetTable():\n",
    "    global qTable_1\n",
    "    qTable_1 = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        qTable_1[i] = [0,0,0,0] \n",
    "    global currentState\n",
    "    currentState = 0\n",
    "    \n",
    "def getPosition(state):\n",
    "    for i in range(len(statePositions)):\n",
    "        for j in range(len(statePositions[i])):\n",
    "            if statePositions[i][j] == state:\n",
    "                return i, j\n",
    "            \n",
    "def calcReward(state, nextState):\n",
    "    y1, x1 = getPosition(state)\n",
    "    y2, x2 = getPosition(nextState)\n",
    "    y3, x3 = getPosition(giftState)\n",
    "    \n",
    "    currentDist = (((y3 - y1)**2)+((x3 - x1)**2))**0.5\n",
    "    nextDist = (((y3 - y2)**2)+((x3 - x2)**2))**0.5\n",
    "    \n",
    "    changeInDist = currentDist-nextDist\n",
    "    return changeInDist/2\n",
    "\n",
    "def calcPossibleMoves(state):\n",
    "    global qTable_1\n",
    "    possibleMoves = []\n",
    "    \n",
    "    if state == 0:\n",
    "        return [1,2]\n",
    "    \n",
    "    if (state+1) % mazeSize[1] != 0:\n",
    "        possibleMoves.append(2)\n",
    "        \n",
    "    if (state+1) % mazeSize[1] != 1:\n",
    "        possibleMoves.append(0)\n",
    "        \n",
    "    if state > (mazeSize[1]-1):\n",
    "        possibleMoves.append(3)\n",
    "    \n",
    "    if state < ((mazeSize[0] * mazeSize[1]) - mazeSize[1]):\n",
    "        possibleMoves.append(1)\n",
    "        \n",
    "    return possibleMoves\n",
    "\n",
    "def nextStep(state):\n",
    "    global qTable_1\n",
    "    possMoves = calcPossibleMoves(state)\n",
    "    \n",
    "    if random.random() < epsilonValue:\n",
    "        nextMove = random.choice(possMoves)\n",
    "    else:\n",
    "        qValues = {}\n",
    "        for move in possMoves:\n",
    "            qValues[move] = qTable_1[state][move]\n",
    "\n",
    "        maxValue = max(qValues.values())\n",
    "        minValue = min(qValues.values())\n",
    "        count_max = sum(1 for value in qValues.values() if value == maxValue)\n",
    "        count_min = sum(1 for value in qValues.values() if value == minValue)\n",
    "\n",
    "        if count_max > 1 and count_max < len(possMoves):\n",
    "            nextMove = random.choice([move for move in possMoves if qValues[move] != minValue])\n",
    "        elif count_max == len(possMoves):\n",
    "            nextMove = random.choice(possMoves)\n",
    "        else:\n",
    "            nextMove = max(qValues, key=qValues.get)\n",
    "    return nextMove\n",
    "\n",
    "def pathFound():\n",
    "    currentState = 0\n",
    "    for i in range(mazeSize[0]*mazeSize[1]-1):\n",
    "        bestDirection = None\n",
    "        if max(qTable_1[currentState]) > 0:\n",
    "            bestDirection = qTable_1[currentState].index(max(qTable_1[currentState]))\n",
    "        newState = 0\n",
    "        if bestDirection == 0 and 0 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - 1\n",
    "        elif bestDirection == 1 and 1 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + mazeSize[1]\n",
    "        elif bestDirection == 2 and 2 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + 1\n",
    "        elif bestDirection == 3 and 3 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - mazeSize[1]\n",
    "\n",
    "        if newState == giftState:\n",
    "            return True\n",
    "        currentState = newState\n",
    "    return False\n",
    "\n",
    "timesVisited = {}\n",
    "def resetVisited():\n",
    "    global timesVisited\n",
    "    timesVisited = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        timesVisited[i] = 0\n",
    "\n",
    "def updateTable_qLearning(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (max(qTable_1[nextState])) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ\n",
    "\n",
    "def updateTable_sarsa(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    nextDirection = nextStep(nextState)\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (qTable_1[nextState][nextDirection]) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ, nextDirection\n",
    "\n",
    "def updateTable_monteCarlo(steps, totalReward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    global direction\n",
    "    minChangeInQ = 0\n",
    "    didConverge = False\n",
    "    avgReward = totalReward/len(steps)\n",
    "    for i in range(len(steps)):\n",
    "        updated = qTable_1[steps[i][0]][steps[i][1]] + alpha*(avgReward-qTable_1[steps[i][0]][steps[i][1]])\n",
    "        changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "        if i == 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        elif changeInQ < minChangeInQ and changeInQ != 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        if changeInQ < convergenceThresh:\n",
    "            if changeInQ > 0 and pathFound():\n",
    "                didConverge = True\n",
    "        qTable_1[steps[i][0]][steps[i][1]] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    return didConverge, minChangeInQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b6b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilonValue = 0.35\n",
    "alpha = 0.65\n",
    "convergenceThresh = 0.01\n",
    "revistPenalty = -0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93fee2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q LEARNING-----------------------------------------------------------------------\n",
    "\n",
    "def qLearning(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global totalTrials\n",
    "    global totalTime_start\n",
    "    \n",
    "    currentState = 0\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "    \n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        direction = nextStep(currentState)\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "        \n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        converged, changeInQ = updateTable_qLearning(direction, nextState, reward)\n",
    "\n",
    "        if terminated or truncated or converged:\n",
    "            observation, info = env.reset()\n",
    "            currentState = 0\n",
    "            if not converged:\n",
    "                currentEpisode += 1\n",
    "                resetVisited()\n",
    "\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"Trial: \" + 'Q-Learning, ' + 'trialType '+ str(trialType) + ', Trial ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Trial Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        print(\"Total Time: \" + str(round((time.time()-totalTimeStart)/60, 3)) + \" min\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeInQ))\n",
    "        \n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return currentEpisode, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07c94f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA-----------------------------------------------------------------------\n",
    "\n",
    "def sarsa(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global totalTrials\n",
    "    global totalTimeStart\n",
    "    \n",
    "    currentState = 0\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "    startOfEpisode = True\n",
    "    direction = None\n",
    "\n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        if startOfEpisode:\n",
    "            direction = nextStep(currentState)\n",
    "            startOfEpisode = False\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "\n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        converged, changeInQ, nextDirection = updateTable_sarsa(direction, nextState, reward)\n",
    "        direction = nextDirection\n",
    "\n",
    "        if terminated or truncated or converged:\n",
    "            observation, info = env.reset()\n",
    "            currentState = 0\n",
    "            if not converged:\n",
    "                currentEpisode += 1\n",
    "                startOfEpisode = False\n",
    "                resetVisited()\n",
    "\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(\"trial: \" + 'SARSA, ' + 'trialType '+ str(trialType) + ', Trial ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Trial Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        print(\"Total Time: \" + str(round((time.time()-totalTimeStart)/60, 3)) + \" min\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeInQ))\n",
    "\n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return currentEpisode, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6db2d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo -----------------------------------------------------------------------\n",
    "\n",
    "def monteCarlo(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global direction\n",
    "    global totalTrials\n",
    "    global totalTimeStart\n",
    "    \n",
    "    maxEpisodes = 1000\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "\n",
    "    episodeSteps = []\n",
    "    totalReward = 0\n",
    "\n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        global currentState\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        direction = nextStep(currentState)\n",
    "        episodeSteps.append([currentState, direction])\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "\n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        totalReward += reward\n",
    "        currentState = nextState\n",
    "\n",
    "        changeQ = 0\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            converged, changeInQ = updateTable_monteCarlo(episodeSteps, totalReward)\n",
    "            changeQ = changeInQ\n",
    "            episodeSteps = []\n",
    "            currentState = 0\n",
    "            currentEpisode += 1\n",
    "            resetVisited()\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        print(\"trial: \" + 'Monte Carlo, ' + 'trialType '+ str(trialType) + ', Trial ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Current Path: \" + str(episodeSteps))\n",
    "        print(\"Trial Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        print(\"Total Time: \" + str(round((time.time()-totalTimeStart)/60, 3)) + \" min\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeQ))\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return currentEpisode, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0c34a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial types: \n",
    "#     0: only +10 reward for reaching present\n",
    "    \n",
    "#     1: gets +10 for reaching present\n",
    "#        possitive reward equal to 1/2 change in distance towards the present\n",
    "#        negative reward equal to 1/2 change in distance away from present\n",
    "#        negative reward for revisiting spaces equal to -0.25 * times visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93b43208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: Q-Learning, trialType 0, Trial 2/3\n",
      "Episode: 845/5000\n",
      "Time: 1176.217 sec\n",
      "Q-Table:\n",
      "0: [0, 0.0, 0.0, 0]\n",
      "1: [0.0, 0.0, 0.0, 0]\n",
      "2: [0.0, 0.0, 0.0, 0]\n",
      "3: [0.0, 0.0, 0, 0]\n",
      "4: [0, 0.0, 0.0, 0.0]\n",
      "5: [0, 0, 0, 0]\n",
      "6: [0, 0, 0, 0]\n",
      "7: [0, 0, 0, 0]\n",
      "8: [0, 0.0, 0.0, 0.0]\n",
      "9: [0.0, 0.0, 0.0, 0.0]\n",
      "10: [0.0, 0.0, 0.0, 0.0]\n",
      "11: [0.0, 2.7462500000000003, 0, 0.0]\n",
      "12: [0, 0, 0, 0]\n",
      "13: [0.0, 0.0, 0.0, 0.0]\n",
      "14: [0, 0, 0, 0]\n",
      "15: [0.0, 4.2250000000000005, 0, 0.0]\n",
      "16: [0, 0, 0, 0.0]\n",
      "17: [0.0, 0, 0, 0.0]\n",
      "18: [0, 0, 0, 0]\n",
      "19: [8.775, 0, 0, 0]\n",
      "change in Q: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alg \u001b[38;5;241m==\u001b[39m algs[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m     10\u001b[0m     episodes1, duration1 \u001b[38;5;241m=\u001b[39m qLearning(typ, maxEpisodes, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     episodes2, duration2 \u001b[38;5;241m=\u001b[39m qLearning(typ, maxEpisodes, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     12\u001b[0m     episodes3, duration3 \u001b[38;5;241m=\u001b[39m qLearning(typ, maxEpisodes, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m alg \u001b[38;5;241m==\u001b[39m algs[\u001b[38;5;241m1\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[44], line 43\u001b[0m, in \u001b[0;36mqLearning\u001b[1;34m(trialType, maxEpisodes, currentTrial)\u001b[0m\n\u001b[0;32m     40\u001b[0m converged, changeInQ \u001b[38;5;241m=\u001b[39m updateTable_qLearning(direction, nextState, reward)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated \u001b[38;5;129;01mor\u001b[39;00m converged:\n\u001b[1;32m---> 43\u001b[0m     observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     44\u001b[0m     currentState \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m converged:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:59\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:322\u001b[0m, in \u001b[0;36mFrozenLakeEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_gui(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:432\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    430\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m    431\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    435\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    436\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxEpisodes = 5000\n",
    "algs = ['Q-Learning', 'SARSA', 'Monte Carlo']\n",
    "trials = [0,1]\n",
    "totalTrials = 3\n",
    "trialData = pd.DataFrame()\n",
    "totalTime_start = time.time()\n",
    "\n",
    "for alg in algs:\n",
    "    for typ in trials:\n",
    "        if alg == algs[0]:\n",
    "            episodes1, duration1 = qLearning(typ, maxEpisodes, 1)\n",
    "            episodes2, duration2 = qLearning(typ, maxEpisodes, 2)\n",
    "            episodes3, duration3 = qLearning(typ, maxEpisodes, 3)\n",
    "        elif alg == algs[1]:\n",
    "            episodes1, duration1 = sarsa(typ, maxEpisodes, 1)\n",
    "            episodes2, duration2 = sarsa(typ, maxEpisodes, 2)\n",
    "            episodes3, duration3 = sarsa(typ, maxEpisodes, 3)\n",
    "        elif alg == algs[2]:\n",
    "            episodes1, duration1 = monteCarlo(typ, maxEpisodes, 1)\n",
    "            episodes2, duration2 = monteCarlo(typ, maxEpisodes, 2)\n",
    "            episodes3, duration3 = monteCarlo(typ, maxEpisodes, 3)\n",
    "        row = pd.DataFrame({'alg':alg,\n",
    "                            'trialType':trial,\n",
    "                            'trial-1_episodes':episodes1,\n",
    "                            'trial-1_duration': duration1,\n",
    "                            'trial-2_episodes':episodes2,\n",
    "                            'trial-2_duration': duration2,\n",
    "                            'trial-3_episodes':episodes3,\n",
    "                            'trial-3_duration': duration3}, index=[0])\n",
    "        trialData = pd.concat([trialData, row], ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82da0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trialData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b0f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
