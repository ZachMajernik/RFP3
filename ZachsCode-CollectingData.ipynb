{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f7f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium renderlab\n",
    "# !pip install opencv-python\n",
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03cf841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "%config NotebookApp.iopub_msg_rate_limit=10000\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d51652",
   "metadata": {},
   "outputs": [],
   "source": [
    "qLearningData = pd.read_csv('Q-LearningData.csv')\n",
    "sarsaData = pd.read_csv('SarsaData.csv')\n",
    "monteCarlo_FV = pd.read_csv('MonteCarlo_FirstVisitData.csv')\n",
    "monteCarlo_EV = pd.read_csv('MonteCarlo_EveryVisitData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d3f34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "#visualise maze:\n",
    "rfpMaze = [\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "maze1 = [\"SFFH\", \"FHHF\", \"FHFG\", \"FFFH\", \"HFHH\"]\n",
    "\n",
    "desc = rfpMaze\n",
    "mazeSize = [len(desc),len(desc[0])]\n",
    "\n",
    "statePositions = [[] for _ in range(mazeSize[0])]\n",
    "stateNum = 0\n",
    "for i in range(mazeSize[0]):\n",
    "    for j in range(mazeSize[1]):\n",
    "        statePositions[i].append(stateNum)\n",
    "        stateNum += 1\n",
    "        \n",
    "\n",
    "        \n",
    "giftState = -1\n",
    "gift_found = False\n",
    "for i in range(len(desc)):\n",
    "    if gift_found:\n",
    "        break\n",
    "    for j in range(len(desc[i])):\n",
    "        giftState += 1\n",
    "        if desc[i][j] == 'G':\n",
    "            gift_found = True\n",
    "            break\n",
    "            \n",
    "print(giftState)\n",
    "print(statePositions)\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x5\", is_slippery=False, render_mode=\"human\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b980268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qTable_1 = {}\n",
    "def resetTable():\n",
    "    global qTable_1\n",
    "    qTable_1 = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        qTable_1[i] = [0,0,0,0] \n",
    "    global currentState\n",
    "    currentState = 0\n",
    "    \n",
    "def getPosition(state):\n",
    "    for i in range(len(statePositions)):\n",
    "        for j in range(len(statePositions[i])):\n",
    "            if statePositions[i][j] == state:\n",
    "                return i, j\n",
    "            \n",
    "def calcReward(state, nextState):\n",
    "    y1, x1 = getPosition(state)\n",
    "    y2, x2 = getPosition(nextState)\n",
    "    y3, x3 = getPosition(giftState)\n",
    "    \n",
    "    currentDist = (((y3 - y1)**2)+((x3 - x1)**2))**0.5\n",
    "    nextDist = (((y3 - y2)**2)+((x3 - x2)**2))**0.5\n",
    "    \n",
    "    changeInDist = currentDist-nextDist\n",
    "    return changeInDist/2\n",
    "\n",
    "def calcPossibleMoves(state):\n",
    "    global qTable_1\n",
    "    possibleMoves = []\n",
    "    \n",
    "    if state == 0:\n",
    "        return [1,2]\n",
    "    \n",
    "    if (state+1) % mazeSize[1] != 0:\n",
    "        possibleMoves.append(2)\n",
    "        \n",
    "    if (state+1) % mazeSize[1] != 1:\n",
    "        possibleMoves.append(0)\n",
    "        \n",
    "    if state > (mazeSize[1]-1):\n",
    "        possibleMoves.append(3)\n",
    "    \n",
    "    if state < ((mazeSize[0] * mazeSize[1]) - mazeSize[1]):\n",
    "        possibleMoves.append(1)\n",
    "        \n",
    "    return possibleMoves\n",
    "\n",
    "def nextStep(state):\n",
    "    global qTable_1\n",
    "    possMoves = calcPossibleMoves(state)\n",
    "    \n",
    "    if random.random() < epsilonValue:\n",
    "        nextMove = random.choice(possMoves)\n",
    "    else:\n",
    "        qValues = {}\n",
    "        for move in possMoves:\n",
    "            qValues[move] = qTable_1[state][move]\n",
    "\n",
    "        maxValue = max(qValues.values())\n",
    "        minValue = min(qValues.values())\n",
    "        count_max = sum(1 for value in qValues.values() if value == maxValue)\n",
    "        count_min = sum(1 for value in qValues.values() if value == minValue)\n",
    "\n",
    "        if count_max > 1 and count_max < len(possMoves):\n",
    "            nextMove = random.choice([move for move in possMoves if qValues[move] != minValue])\n",
    "        elif count_max == len(possMoves):\n",
    "            nextMove = random.choice(possMoves)\n",
    "        else:\n",
    "            nextMove = max(qValues, key=qValues.get)\n",
    "    return nextMove\n",
    "\n",
    "def pathFound():\n",
    "    currentState = 0\n",
    "    for i in range(mazeSize[0]*mazeSize[1]-1):\n",
    "        bestDirection = None\n",
    "        if max(qTable_1[currentState]) > 0:\n",
    "            bestDirection = qTable_1[currentState].index(max(qTable_1[currentState]))\n",
    "        newState = 0\n",
    "        if bestDirection == 0 and 0 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - 1\n",
    "        elif bestDirection == 1 and 1 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + mazeSize[1]\n",
    "        elif bestDirection == 2 and 2 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + 1\n",
    "        elif bestDirection == 3 and 3 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - mazeSize[1]\n",
    "\n",
    "        if newState == giftState:\n",
    "            return True\n",
    "        currentState = newState\n",
    "    return False\n",
    "\n",
    "timesVisited = {}\n",
    "def resetVisited():\n",
    "    global timesVisited\n",
    "    timesVisited = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        timesVisited[i] = 0\n",
    "\n",
    "def updateTable_qLearning(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (max(qTable_1[nextState])) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ\n",
    "\n",
    "def updateTable_sarsa(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    nextDirection = nextStep(nextState)\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (qTable_1[nextState][nextDirection]) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ, nextDirection\n",
    "\n",
    "def updateTable_monteCarlo(steps, totalReward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    global direction\n",
    "    minChangeInQ = 0\n",
    "    didConverge = False\n",
    "    avgReward = totalReward/len(steps)\n",
    "    for i in range(len(steps)):\n",
    "        updated = qTable_1[steps[i][0]][steps[i][1]] + alpha*(avgReward-qTable_1[steps[i][0]][steps[i][1]])\n",
    "        changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "        if i == 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        elif changeInQ < minChangeInQ and changeInQ != 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        if changeInQ < convergenceThresh:\n",
    "            if changeInQ > 0 and pathFound():\n",
    "                didConverge = True\n",
    "        qTable_1[steps[i][0]][steps[i][1]] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    return didConverge, minChangeInQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b6b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilonValue = 0.35\n",
    "alpha = 0.65\n",
    "convergenceThresh = 0.01\n",
    "revistPenalty = -0.25\n",
    "totalTrials = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93fee2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q LEARNING-----------------------------------------------------------------------\n",
    "\n",
    "def qLearning(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global totalTrials\n",
    "    global totalTime_start\n",
    "    \n",
    "    currentState = 0\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "    \n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        direction = nextStep(currentState)\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "        \n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        converged, changeInQ = updateTable_qLearning(direction, nextState, reward)\n",
    "\n",
    "        if terminated or truncated or converged:\n",
    "            observation, info = env.reset()\n",
    "            currentState = 0\n",
    "            if not converged:\n",
    "                currentEpisode += 1\n",
    "                resetVisited()\n",
    "\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        if totalTrials != 0:\n",
    "            print(\"Trial: \" + 'Q-Learning, ' + 'trialType '+ str(trialType) + ', Trial ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Trial Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        if totalTrials != 0:\n",
    "            print(\"Total Time: \" + str(round((time.time()-totalTime_start)/60, 3)) + \" min\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeInQ))\n",
    "        \n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return (currentEpisode-1), duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c94f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA-----------------------------------------------------------------------\n",
    "\n",
    "def sarsa(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global totalTrials\n",
    "    global totalTime_start\n",
    "    \n",
    "    currentState = 0\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "    startOfEpisode = True\n",
    "    direction = None\n",
    "\n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        if startOfEpisode:\n",
    "            direction = nextStep(currentState)\n",
    "            startOfEpisode = False\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "\n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        converged, changeInQ, nextDirection = updateTable_sarsa(direction, nextState, reward)\n",
    "        direction = nextDirection\n",
    "\n",
    "        if terminated or truncated or converged:\n",
    "            observation, info = env.reset()\n",
    "            currentState = 0\n",
    "            if not converged:\n",
    "                currentEpisode += 1\n",
    "                startOfEpisode = False\n",
    "                resetVisited()\n",
    "\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        if totalTrials != 0:\n",
    "            print(\"trial: \" + 'SARSA, ' + 'trialType '+ str(trialType) + ', Trial ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Trial Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        if totalTrials != 0:\n",
    "            print(\"Total Time: \" + str(round((time.time()-totalTime_start)/60, 3)) + \" min\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeInQ))\n",
    "\n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return (currentEpisode-1), duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db2d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo -----------------------------------------------------------------------\n",
    "\n",
    "def monteCarlo_firstVisit(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global direction\n",
    "    global totalTrials\n",
    "    global totalTime_start\n",
    "    \n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "\n",
    "    episodeSteps = []\n",
    "    totalEpisodeReward = 0\n",
    "\n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        global currentState\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        direction = nextStep(currentState)\n",
    "        if not [currentState, direction] in episodeSteps:\n",
    "            episodeSteps.append([currentState, direction])\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "\n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        totalEpisodeReward += reward\n",
    "        currentState = nextState\n",
    "\n",
    "        changeQ = 0\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            converged, changeInQ = updateTable_monteCarlo(episodeSteps, totalEpisodeReward)\n",
    "            changeQ = changeInQ\n",
    "            episodeSteps = []\n",
    "            currentState = 0\n",
    "            totalEpisodeReward = 0\n",
    "            currentEpisode += 1\n",
    "            resetVisited()\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        if totalTrials != 0:\n",
    "            print(\"trial: \" + 'Monte Carlo, ' + 'trialType '+ str(trialType) + ', Trial ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"States Visited: \" + str(episodeSteps))\n",
    "        print(\"Trial Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        if totalTrials != 0:\n",
    "            print(\"Total Time: \" + str(round((time.time()-totalTime_start)/60, 3)) + \" min\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeQ))\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return (currentEpisode-1), duration\n",
    "\n",
    "def monteCarlo_everyVisit(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global direction\n",
    "    global totalTrials\n",
    "    global totalTime_start\n",
    "    \n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "\n",
    "    episodeSteps = []\n",
    "    totalEpisodeReward = 0\n",
    "\n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        global currentState\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        direction = nextStep(currentState)\n",
    "        episodeSteps.append([currentState, direction])\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "\n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        totalEpisodeReward += reward\n",
    "        currentState = nextState\n",
    "\n",
    "        changeQ = 0\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            converged, changeInQ = updateTable_monteCarlo(episodeSteps, totalEpisodeReward)\n",
    "            changeQ = changeInQ\n",
    "            episodeSteps = []\n",
    "            currentState = 0\n",
    "            totalEpisodeReward = 0\n",
    "            currentEpisode += 1\n",
    "            resetVisited()\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        if totalTrials != 0:\n",
    "            print(\"trial: \" + 'Monte Carlo, ' + 'trialType '+ str(trialType) + ', Trial ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"States Visited: \" + str(episodeSteps))\n",
    "        print(\"Trial Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        if totalTrials != 0:\n",
    "            print(\"Total Time: \" + str(round((time.time()-totalTime_start)/60, 3)) + \" min\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeQ))\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return (currentEpisode-1), duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b9b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial types: \n",
    "#     0: only +10 reward for reaching present\n",
    "    \n",
    "#     1: gets +10 for reaching present\n",
    "#        possitive reward equal to 1/2 change in distance towards the present\n",
    "#        negative reward equal to 1/2 change in distance away from present\n",
    "#        negative reward for revisiting spaces equal to -0.25 * times visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94af4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trialData = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90451749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: Monte Carlo, trialType 1, Trial 10/10\n",
      "Episode: 230/2000\n",
      "States Visited: []\n",
      "Trial Time: 449.848 sec\n",
      "Total Time: 146.237 min\n",
      "Q-Table:\n",
      "0: [0, 0.03504484305972627, -0.2020298328976209, 0]\n",
      "1: [-0.12793772865959296, -0.33988556414293697, -0.16080437493174154, 0]\n",
      "2: [-0.15485254698481926, -0.22102637175016138, -0.15260985020754314, 0]\n",
      "3: [-0.139276277729119, -0.15269332539984667, 0, 0]\n",
      "4: [0, 0.33044661209351245, -0.1094613007383318, -0.029558912665251597]\n",
      "5: [0, 0, 0, 0]\n",
      "6: [0, 0, 0, 0]\n",
      "7: [0, 0, 0, 0]\n",
      "8: [0, -0.05490106642484118, 0.38634691482595385, 0.0019252947334270631]\n",
      "9: [0.46684421168510193, 1.2812571827229684, 0.03308663041346868, 0.02566230416282965]\n",
      "10: [0.020428151136896562, 0.033138637850322825, 0.01479640358971897, 0]\n",
      "11: [0, 0, 0, 0.01479640358971897]\n",
      "12: [0, 0, 0, 0]\n",
      "13: [0.10438514139163489, 1.5824342437802885, 0.0963088912593783, 0.3600364507515853]\n",
      "14: [0, 0, 0, 0]\n",
      "15: [0, 0, 0, 0]\n",
      "16: [0, 0, 1.186029179846962, 0.030142192164574724]\n",
      "17: [0.07095083220287017, 0, 1.5827117642777584, 0.7808952193616756]\n",
      "18: [0, 0, 0, 0]\n",
      "19: [0, 0, 0, 0]\n",
      "change in Q: 0.00193\n",
      "449.848 seconds to converge\n"
     ]
    }
   ],
   "source": [
    "# maxEpisodes = 2000\n",
    "# trials = [0,1]\n",
    "# totalTrials = 10\n",
    "# totalTime_start = time.time()\n",
    "\n",
    "# for i in range(totalTrials):\n",
    "#     episodes, duration = monteCarlo(0, maxEpisodes, i+1)\n",
    "#     row = pd.DataFrame({'alg': 'Monte Carlo',\n",
    "#                         'trialType': 0,\n",
    "#                         'trial_index': i,\n",
    "#                         'episodes': episodes,\n",
    "#                         'duration':duration,}, index=[0])\n",
    "#     trialData = pd.concat([trialData, row], ignore_index=True)\n",
    "#     trialData.to_csv('monteCarloData1.csv', index=False)\n",
    "\n",
    "# for i in range(totalTrials):\n",
    "#     episodes, duration = monteCarlo(1, maxEpisodes, i+1)\n",
    "#     row = pd.DataFrame({'alg': 'Monte Carlo',\n",
    "#                         'trialType': 1,\n",
    "#                         'trial_index': i,\n",
    "#                         'episodes': episodes,\n",
    "#                         'duration':duration,}, index=[0])\n",
    "#     trialData = pd.concat([trialData, row], ignore_index=True)\n",
    "#     trialData.to_csv('monteCarloData1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82da0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286431a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qLearningData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94896609",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"q-learning trial type 0 min: \" + str(min(qLearningData.loc[qLearningData.loc[:,'trialType'] == 0]['duration'])))\n",
    "print(\"q-learning trial type 0 mean: \" + str(qLearningData.loc[qLearningData.loc[:,'trialType'] == 0]['duration'].mean()))\n",
    "print(\"q-learning trial type 1 min: \" + str(min(qLearningData.loc[qLearningData.loc[:,'trialType'] == 1]['duration'])))\n",
    "print(\"q-learning trial type 1 mean: \" + str(qLearningData.loc[qLearningData.loc[:,'trialType'] == 1]['duration'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e797ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sarsa trial type 0 min: \" + str(min(sarsaData.loc[sarsaData.loc[:,'trialType'] == 0]['duration'])))\n",
    "print(\"sarsa trial type 0 mean: \" + str(sarsaData.loc[sarsaData.loc[:,'trialType'] == 0]['duration'].mean()))\n",
    "print(\"sarsa trial type 1 min: \" + str(min(sarsaData.loc[sarsaData.loc[:,'trialType'] == 1]['duration'])))\n",
    "print(\"sarsa trial type 1 mean: \" + str(sarsaData.loc[sarsaData.loc[:,'trialType'] == 1]['duration'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7407de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(qLearningData.loc[qLearningData.loc[:,'trialType'] == 0]['duration'], bins=50, kde=True)\n",
    "plt.show()\n",
    "sns.histplot(qLearningData.loc[qLearningData.loc[:,'trialType'] == 1]['duration'], bins=50, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sarsaData.loc[sarsaData.loc[:,'trialType'] == 0]['duration'], bins=50, kde=True)\n",
    "plt.show()\n",
    "sns.histplot(sarsaData.loc[sarsaData.loc[:,'trialType'] == 1]['duration'], bins=50, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b80feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "03a0d00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaef184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
