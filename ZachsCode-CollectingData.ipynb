{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f7f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium renderlab\n",
    "# !pip install opencv-python\n",
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74465ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "%config NotebookApp.iopub_msg_rate_limit=10000\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d3f34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "#visualise maze:\n",
    "rfpMaze = [\"SFFF\", \"FHHH\", \"FFFF\", \"HFHF\", \"FFGF\"]\n",
    "maze1 = [\"SFFH\", \"FHHF\", \"FHFG\", \"FFFH\", \"HFHH\"]\n",
    "\n",
    "desc = rfpMaze\n",
    "mazeSize = [len(desc),len(desc[0])]\n",
    "\n",
    "statePositions = [[] for _ in range(mazeSize[0])]\n",
    "stateNum = 0\n",
    "for i in range(mazeSize[0]):\n",
    "    for j in range(mazeSize[1]):\n",
    "        statePositions[i].append(stateNum)\n",
    "        stateNum += 1\n",
    "        \n",
    "\n",
    "        \n",
    "giftState = -1\n",
    "gift_found = False\n",
    "for i in range(len(desc)):\n",
    "    if gift_found:\n",
    "        break\n",
    "    for j in range(len(desc[i])):\n",
    "        giftState += 1\n",
    "        if desc[i][j] == 'G':\n",
    "            gift_found = True\n",
    "            break\n",
    "            \n",
    "print(giftState)\n",
    "print(statePositions)\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"5x5\", is_slippery=False, render_mode=\"human\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b980268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qTable_1 = {}\n",
    "def resetTable():\n",
    "    global qTable_1\n",
    "    qTable_1 = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        qTable_1[i] = [0,0,0,0] \n",
    "    global currentState\n",
    "    currentState = 0\n",
    "    \n",
    "def getPosition(state):\n",
    "    for i in range(len(statePositions)):\n",
    "        for j in range(len(statePositions[i])):\n",
    "            if statePositions[i][j] == state:\n",
    "                return i, j\n",
    "            \n",
    "def calcReward(state, nextState):\n",
    "    y1, x1 = getPosition(state)\n",
    "    y2, x2 = getPosition(nextState)\n",
    "    y3, x3 = getPosition(giftState)\n",
    "    \n",
    "    currentDist = (((y3 - y1)**2)+((x3 - x1)**2))**0.5\n",
    "    nextDist = (((y3 - y2)**2)+((x3 - x2)**2))**0.5\n",
    "    \n",
    "    changeInDist = currentDist-nextDist\n",
    "    return changeInDist/2\n",
    "\n",
    "def calcPossibleMoves(state):\n",
    "    global qTable_1\n",
    "    possibleMoves = []\n",
    "    \n",
    "    if state == 0:\n",
    "        return [1,2]\n",
    "    \n",
    "    if (state+1) % mazeSize[1] != 0:\n",
    "        possibleMoves.append(2)\n",
    "        \n",
    "    if (state+1) % mazeSize[1] != 1:\n",
    "        possibleMoves.append(0)\n",
    "        \n",
    "    if state > (mazeSize[1]-1):\n",
    "        possibleMoves.append(3)\n",
    "    \n",
    "    if state < ((mazeSize[0] * mazeSize[1]) - mazeSize[1]):\n",
    "        possibleMoves.append(1)\n",
    "        \n",
    "    return possibleMoves\n",
    "\n",
    "def nextStep(state):\n",
    "    global qTable_1\n",
    "    possMoves = calcPossibleMoves(state)\n",
    "    \n",
    "    if random.random() < epsilonValue:\n",
    "        nextMove = random.choice(possMoves)\n",
    "    else:\n",
    "        qValues = {}\n",
    "        for move in possMoves:\n",
    "            qValues[move] = qTable_1[state][move]\n",
    "\n",
    "        maxValue = max(qValues.values())\n",
    "        minValue = min(qValues.values())\n",
    "        count_max = sum(1 for value in qValues.values() if value == maxValue)\n",
    "        count_min = sum(1 for value in qValues.values() if value == minValue)\n",
    "\n",
    "        if count_max > 1 and count_max < len(possMoves):\n",
    "            nextMove = random.choice([move for move in possMoves if qValues[move] != minValue])\n",
    "        elif count_max == len(possMoves):\n",
    "            nextMove = random.choice(possMoves)\n",
    "        else:\n",
    "            nextMove = max(qValues, key=qValues.get)\n",
    "    return nextMove\n",
    "\n",
    "def pathFound():\n",
    "    currentState = 0\n",
    "    for i in range(mazeSize[0]*mazeSize[1]-1):\n",
    "        bestDirection = None\n",
    "        if max(qTable_1[currentState]) > 0:\n",
    "            bestDirection = qTable_1[currentState].index(max(qTable_1[currentState]))\n",
    "        newState = 0\n",
    "        if bestDirection == 0 and 0 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - 1\n",
    "        elif bestDirection == 1 and 1 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + mazeSize[1]\n",
    "        elif bestDirection == 2 and 2 in calcPossibleMoves(currentState):\n",
    "            newState = currentState + 1\n",
    "        elif bestDirection == 3 and 3 in calcPossibleMoves(currentState):\n",
    "            newState = currentState - mazeSize[1]\n",
    "\n",
    "        if newState == giftState:\n",
    "            return True\n",
    "        currentState = newState\n",
    "    return False\n",
    "\n",
    "timesVisited = {}\n",
    "def resetVisited():\n",
    "    global timesVisited\n",
    "    timesVisited = {}\n",
    "    for i in range(mazeSize[0]*mazeSize[1]):\n",
    "        timesVisited[i] = 0\n",
    "\n",
    "def updateTable_qLearning(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (max(qTable_1[nextState])) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ\n",
    "\n",
    "def updateTable_sarsa(direction, nextState, reward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    didConverge = False\n",
    "    nextDirection = nextStep(nextState)\n",
    "    updated = qTable_1[currentState][direction] + alpha*(reward + (qTable_1[nextState][nextDirection]) - qTable_1[currentState][direction])\n",
    "    changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "    if changeInQ < convergenceThresh:\n",
    "        if changeInQ > 0 and pathFound():\n",
    "            didConverge = True\n",
    "    qTable_1[currentState][direction] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    currentState = nextState\n",
    "    return didConverge, changeInQ, nextDirection\n",
    "\n",
    "def updateTable_monteCarlo(steps, totalReward):\n",
    "    global qTable_1\n",
    "    global currentState\n",
    "    global direction\n",
    "    minChangeInQ = 0\n",
    "    didConverge = False\n",
    "    avgReward = totalReward/len(steps)\n",
    "    for i in range(len(steps)):\n",
    "        updated = qTable_1[steps[i][0]][steps[i][1]] + alpha*(avgReward-qTable_1[steps[i][0]][steps[i][1]])\n",
    "        changeInQ = round(abs(qTable_1[currentState][direction] - updated),5)\n",
    "        if i == 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        elif changeInQ < minChangeInQ and changeInQ != 0:\n",
    "            minChangeInQ = changeInQ\n",
    "        if changeInQ < convergenceThresh:\n",
    "            if changeInQ > 0 and pathFound():\n",
    "                didConverge = True\n",
    "        qTable_1[steps[i][0]][steps[i][1]] = updated\n",
    "    timesVisited[currentState] += 1\n",
    "    return didConverge, minChangeInQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b6b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilonValue = 0.35\n",
    "alpha = 0.65\n",
    "convergenceThresh = 0.01\n",
    "revistPenalty = -0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fee2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q LEARNING-----------------------------------------------------------------------\n",
    "\n",
    "def qLearning(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global totalTrials\n",
    "    \n",
    "    currentState = 0\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "    \n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        direction = nextStep(currentState)\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "        \n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "            if reward < 0:\n",
    "                reward = 0\n",
    "        elif trialType == 2:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        converged, changeInQ = updateTable_qLearning(direction, nextState, reward)\n",
    "\n",
    "        if terminated or truncated or converged:\n",
    "            observation, info = env.reset()\n",
    "            currentState = 0\n",
    "            if not converged:\n",
    "                currentEpisode += 1\n",
    "                resetVisited()\n",
    "\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(\"trial: \" + 'Q-Learning, ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeInQ))\n",
    "        \n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return currentEpisode, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c94f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA-----------------------------------------------------------------------\n",
    "\n",
    "def sarsa(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global totalTrials\n",
    "    \n",
    "    currentState = 0\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "    startOfEpisode = True\n",
    "    direction = None\n",
    "\n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        if startOfEpisode:\n",
    "            direction = nextStep(currentState)\n",
    "            startOfEpisode = False\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "\n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "            if reward < 0:\n",
    "                reward = 0\n",
    "        elif trialType == 2:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        converged, changeInQ, nextDirection = updateTable_sarsa(direction, nextState, reward)\n",
    "        direction = nextDirection\n",
    "\n",
    "        if terminated or truncated or converged:\n",
    "            observation, info = env.reset()\n",
    "            currentState = 0\n",
    "            if not converged:\n",
    "                currentEpisode += 1\n",
    "                startOfEpisode = False\n",
    "                resetVisited()\n",
    "\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(\"trial: \" + 'SARSA, ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeInQ))\n",
    "\n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return currentEpisode, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db2d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo -----------------------------------------------------------------------\n",
    "\n",
    "def monteCarlo(trialType, maxEpisodes, currentTrial):\n",
    "    global currentState\n",
    "    global revisitPenalty\n",
    "    global direction\n",
    "    global totalTrials\n",
    "    \n",
    "    maxEpisodes = 1000\n",
    "    currentEpisode = 1\n",
    "    converged = False\n",
    "\n",
    "    episodeSteps = []\n",
    "    totalReward = 0\n",
    "\n",
    "    resetVisited()\n",
    "    resetTable()\n",
    "    env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while currentEpisode <= maxEpisodes:\n",
    "        global currentState\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "        direction = nextStep(currentState)\n",
    "        episodeSteps.append([currentState, direction])\n",
    "        nextState, reward, terminated, truncated, info = env.step(direction)\n",
    "\n",
    "        if trialType == 0:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "        elif trialType == 1:\n",
    "            if terminated:\n",
    "                if not reward < 1:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "            if reward < 0:\n",
    "                reward = 0\n",
    "        elif trialType == 2:\n",
    "            if terminated:\n",
    "                if reward < 1:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 10\n",
    "            if not terminated:\n",
    "                reward = calcReward(currentState, nextState)\n",
    "                if timesVisited[nextState] > 0:\n",
    "                    reward += revistPenalty*timesVisited[nextState]\n",
    "\n",
    "        totalReward += reward\n",
    "        currentState = nextState\n",
    "\n",
    "        changeQ = 0\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            converged, changeInQ = updateTable_monteCarlo(episodeSteps, totalReward)\n",
    "            changeQ = changeInQ\n",
    "            episodeSteps = []\n",
    "            currentState = 0\n",
    "            currentEpisode += 1\n",
    "            resetVisited()\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        print(\"trial: \" + 'Monte Carlo, ' + str(currentTrial) + \"/\" + str(totalTrials))\n",
    "        print(\"Episode: \" + str(currentEpisode) + \"/\" + str(maxEpisodes))\n",
    "        print(\"Current Path: \" + str(episodeSteps))\n",
    "        print(\"Time: \" + str(round(time.time()-start_time, 3)) + \" sec\")\n",
    "        print(\"Q-Table:\")\n",
    "        for i in range(len(qTable_1)):\n",
    "            print(str(i) + \": \" + str(qTable_1[i]))\n",
    "        print(\"change in Q: \" + str(changeQ))\n",
    "\n",
    "        if converged:\n",
    "            end_time = time.time()\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    duration = 0\n",
    "    if converged:\n",
    "        duration = end_time - start_time\n",
    "        print(str(round(duration, 3)) + \" seconds to converge\")\n",
    "    else:\n",
    "        duration = None\n",
    "        print(\"No convergence\")\n",
    "        \n",
    "    return currentEpisode, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b450de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial types: \n",
    "#     0: only +10 reward for reaching present\n",
    "        \n",
    "#     1: gets +10 for reaching present\n",
    "#        possitive reward equal to 1/2 change in distance towards the present\n",
    "    \n",
    "#     2: gets +10 for reaching present\n",
    "#        possitive reward equal to 1/2 change in distance towards the present\n",
    "#        negative reward equal to 1/2 change in distance away from present\n",
    "#        negative reward for revisiting spaces proportional to the times visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff324535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: Q-Learning, 1/3\n",
      "Episode: 75/2000\n",
      "Time: 99.263 sec\n",
      "Q-Table:\n",
      "0: [0, 0.0, 0.0, 0]\n",
      "1: [0.0, 0.0, 0.0, 0]\n",
      "2: [0.0, 0.0, 0.0, 0]\n",
      "3: [0.0, 0.0, 0, 0]\n",
      "4: [0, 0.0, 0.0, 0.0]\n",
      "5: [0, 0, 0, 0]\n",
      "6: [0, 0, 0, 0]\n",
      "7: [0, 0, 0, 0]\n",
      "8: [0, 0.0, 0.0, 0.0]\n",
      "9: [0.0, 0.0, 0.0, 0.0]\n",
      "10: [0, 0, 0, 0.0]\n",
      "11: [0, 0, 0, 0]\n",
      "12: [0, 0, 0, 0]\n",
      "13: [0.0, 0.0, 0.0, 0]\n",
      "14: [0, 0, 0, 0]\n",
      "15: [0, 0, 0, 0]\n",
      "16: [0, 0, 0, 0]\n",
      "17: [0, 0, 6.5, 0]\n",
      "18: [0, 0, 0, 0]\n",
      "19: [0, 0, 0, 0]\n",
      "change in Q: 0.0\n"
     ]
    }
   ],
   "source": [
    "maxEpisodes = 2000\n",
    "algs = ['Q-Learning', 'SARSA', 'Monte Carlo']\n",
    "trials = [0,1,2]\n",
    "totalTrials = 3\n",
    "trialData = pd.DataFrame()\n",
    "\n",
    "for alg in algs:\n",
    "    for trial in trials:\n",
    "        if alg == algs[0]:\n",
    "            episodes1, duration1 = qLearning(trial, maxEpisodes, 1)\n",
    "            episodes2, duration2 = qLearning(trial, maxEpisodes, 2)\n",
    "            episodes3, duration3 = qLearning(trial, maxEpisodes, 3)\n",
    "        elif alg == algs[1]:\n",
    "            episodes1, duration1 = sarsa(trial, maxEpisodes, 1)\n",
    "            episodes2, duration2 = sarsa(trial, maxEpisodes, 2)\n",
    "            episodes3, duration3 = sarsa(trial, maxEpisodes, 3)\n",
    "        elif alg == algs[2]:\n",
    "            episodes1, duration1 = monteCarlo(trial, maxEpisodes, 1)\n",
    "            episodes2, duration2 = monteCarlo(trial, maxEpisodes, 2)\n",
    "            episodes3, duration3 = monteCarlo(trial, maxEpisodes, 3)\n",
    "        row = pd.Series({'alg':alg,\n",
    "                            'trialType':trial,\n",
    "                            'trial-1_episodes':episodes1,\n",
    "                            'trial-1_duration': duration1,\n",
    "                            'trial-2_episodes':episodes2,\n",
    "                            'trial-2_duration': duration2,\n",
    "                            'trial-3_episodes':episodes3,\n",
    "                            'trial-3_duration': duration3})\n",
    "        trialData = trialData.append(row, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82da0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trialData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
